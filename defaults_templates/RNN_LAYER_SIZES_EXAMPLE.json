{
    "_comment_purpose": "Demonstrates new RNN_LAYER_SIZES parameter for LSTM/GRU architectures with per-layer hidden units",
    "_comment_usage": "This file shows how to specify variable-size recurrent layers using comma-separated sizes",
    
    "FEATURE_COLUMNS": ["Power", "Battery_Temp_degC", "SOC"],
    "TARGET_COLUMN": "Voltage",
    "MODEL_TYPE": "LSTM",
    "TRAINING_METHOD": "Sequence-to-Sequence",
    
    "_comment_new_param": "=== NEW PARAMETER: RNN_LAYER_SIZES ===",
    "RNN_LAYER_SIZES": "64,32",
    "_comment_compat": "Legacy LAYERS and HIDDEN_UNITS still work for uniform layer sizes",
    
    "_comment_examples": "=== RNN_LAYER_SIZES Examples ===",
    "_example_1": "RNN_LAYER_SIZES: '64,32' creates 2 layers with 64 then 32 hidden units",
    "_example_2": "RNN_LAYER_SIZES: '128,64,32' creates 3 layers with decreasing sizes",
    "_example_3": "RNN_LAYER_SIZES: '16,32,16' creates 3 layers with non-monotonic sizes",
    "_example_4": "RNN_LAYER_SIZES: '64,64,64' is equivalent to LAYERS=3, HIDDEN_UNITS=64",
    "_example_5": "For GRU use GRU_UNITS: '64,32' (same format)",
    
    "_comment_training": "=== TRAINING PARAMETERS ===",
    "BATCH_SIZE": "256",
    "LOOKBACK": "200",
    "MAX_EPOCHS": "100",
    "INITIAL_LR": "0.001",
    "SCHEDULER_TYPE": "StepLR",
    "LR_PARAM": "0.8",
    "LR_PERIOD": "20",
    "VALID_PATIENCE": "20",
    "VALID_FREQUENCY": "1",
    "REPETITIONS": 1,
    
    "_comment_optuna": "=== For Optuna grid/random search, specify ranges ===",
    "_optuna_example_1": "RNN_LAYER_SIZES with categorical choices: ['32,16', '64,32', '128,64,32']",
    "_optuna_example_2": "Still supports legacy: LAYERS: [1,3], HIDDEN_UNITS: [16,128] for uniform",
    
    "_comment_system": "=== SYSTEM ===",
    "DEVICE_SELECTION": "cuda:0",
    "OPTIMIZER_TYPE": "Adam",
    "USE_MIXED_PRECISION": true,
    "NUM_WORKERS": "4",
    "PIN_MEMORY": true,
    "PREFETCH_FACTOR": "2",
    "MAX_TRAIN_HOURS": "0",
    "MAX_TRAIN_MINUTES": "0",
    "MAX_TRAIN_SECONDS": "0"
}
