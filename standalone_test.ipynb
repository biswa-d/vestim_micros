{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standalone Model Inference Script\n",
    "\n",
    "This notebook allows you to run inference with a pre-trained PyTorch model (FNN, GRU, or LSTM) from the `vestim` project on a new dataset.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Fill in the `JOB_FOLDER_PATH`, `MODEL_FOLDER_PATH`, and `TEST_DATA_PATH` in the 'Configuration Parameters' cell below.\n",
    "2.  The script will automatically find the correct exported model, and scaler from the job folder.\n",
    "3.  If your model requires features that were generated during training (e.g., filtered columns), use the optional 'Data Augmentation' section to create them.\n",
    "4.  The script will validate that all necessary columns are present and provide a clear error if columns are missing.\n",
    "5.  Run the cells sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Class Definitions\n",
    "\n",
    "These are the model definitions copied from the `vestim` project to ensure this notebook is self-contained. Make sure these match the definitions used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layer_sizes_str, activation_str='ReLU', dropout_prob=0.0):\n",
    "        super(FNNModel, self).__init__()\n",
    "        hidden_layer_sizes = [int(size.strip()) for size in hidden_layer_sizes_str.split(',')]\n",
    "        \n",
    "        layers = []\n",
    "        current_dim = input_size\n",
    "        \n",
    "        activation_fn = getattr(nn, activation_str, nn.ReLU)\n",
    "\n",
    "        for hidden_dim in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            layers.append(activation_fn())\n",
    "            if dropout_prob > 0:\n",
    "                layers.append(nn.Dropout(dropout_prob))\n",
    "            current_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(current_dim, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 3:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        return self.network(x)\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, num_layers, output_size=1, device='cpu'):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_units, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, output_size)\n",
    "\n",
    "    def forward(self, x, h_0=None):\n",
    "        if h_0 is None:\n",
    "            h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_units).to(self.device)\n",
    "        out, _ = self.gru(x, h_0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, num_layers, output_size=1, device='cpu'):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_units, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, output_size)\n",
    "\n",
    "    def forward(self, x, h_s=None, h_c=None):\n",
    "        if h_s is None or h_c is None:\n",
    "            h_s = torch.zeros(self.num_layers, x.size(0), self.hidden_units).to(self.device)\n",
    "            h_c = torch.zeros(self.num_layers, x.size(0), self.hidden_units).to(self.device)\n",
    "        out, _ = self.lstm(x, (h_s, h_c))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REQUIRED PATHS ---\n",
    "JOB_FOLDER_PATH = \"output/job_20250902-141234\"      # Path to the root job folder from your training run\n",
    "MODEL_FOLDER_PATH = \"output/job_20250902-141234/models/FNN_64_32/B4096_LR_SLR_VP120_rep_3\" # Path to the specific model folder containing 'best_model_export.pt'\n",
    "TEST_DATA_PATH = \"path/to/your/test_data.csv\"      # Path to your new test CSV file\n",
    "# --- END OF REQUIRED PATHS ---\n",
    "\n",
    "# --- DERIVED PATHS (DO NOT MODIFY) ---\n",
    "EXPORTED_MODEL_PATH = os.path.join(MODEL_FOLDER_PATH, 'best_model_export.pt')\n",
    "SCALER_PATH = os.path.join(JOB_FOLDER_PATH, 'scalers', 'augmentation_scalers.joblib')\n",
    "JOB_METADATA_PATH = os.path.join(JOB_FOLDER_PATH, 'job_metadata.json')\n",
    "\n",
    "# Device Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_butterworth_filter(df, column_name, new_column_name, filter_order, sampling_freq, corner_freq):\n",
    "    \"\"\"Applies a Butterworth low-pass filter to a column and adds it to the DataFrame.\"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "    \n",
    "    # Butterworth filter design\n",
    "    nyquist = 0.5 * sampling_freq\n",
    "    if corner_freq >= nyquist:\n",
    "        raise ValueError(f\"Corner frequency ({corner_freq}Hz) must be less than the Nyquist frequency ({nyquist}Hz). Adjust your filter settings.\")\n",
    "        \n",
    "    normal_corner = corner_freq / nyquist\n",
    "    b, a = butter(filter_order, normal_corner, btype='low', analog=False)\n",
    "    \n",
    "    # Apply the filter\n",
    "    filtered_data = filtfilt(b, a, df[column_name])\n",
    "    \n",
    "    # Add the new column to the DataFrame\n",
    "    df[new_column_name] = filtered_data\n",
    "    print(f\"Created filtered column '{new_column_name}' from '{column_name}'.\")\n",
    "    return df\n",
    "\n",
    "def create_sequences(data, lookback):\n",
    "    \"\"\"Creates sequences from a numpy array.\"\"\"\n",
    "    X = []\n",
    "    for i in range(len(data) - lookback + 1):\n",
    "        X.append(data[i:(i + lookback)])\n",
    "    return np.array(X)\n",
    "\n",
    "def inverse_transform_single_column(data, scaler, column_name, all_columns):\n",
    "    \"\"\"Inverse transforms a single column of data using a multi-feature scaler.\"\"\"\n",
    "    try:\n",
    "        col_index = all_columns.index(column_name)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the list of scaled columns.\")\n",
    "    \n",
    "    dummy_array = np.zeros((len(data), len(all_columns)))\n",
    "    dummy_array[:, col_index] = data.flatten()\n",
    "    \n",
    "    inversed_data = scaler.inverse_transform(dummy_array)\n",
    "    return inversed_data[:, col_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Inference Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Model Checkpoint and Configuration ---\n",
    "print(\"--- Loading Model Checkpoint and Configuration ---\")\n",
    "try:\n",
    "    checkpoint = torch.load(EXPORTED_MODEL_PATH, map_location=DEVICE)\n",
    "    print(f\"Successfully loaded model checkpoint from {EXPORTED_MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    raise IOError(f\"Error loading model checkpoint file: {e}\")\n",
    "\n",
    "model_hyperparams = checkpoint.get('hyperparams', {})\n",
    "data_config = checkpoint.get('data_config', {})\n",
    "MODEL_TYPE = checkpoint.get('model_type')\n",
    "FEATURE_COLUMNS = data_config.get('feature_columns')\n",
    "TARGET_COLUMN = data_config.get('target_column')\n",
    "LOOKBACK = int(data_config.get('lookback', 0))\n",
    "TRAINING_METHOD = checkpoint.get('model_metadata', {}).get('training_method')\n",
    "\n",
    "# --- 2. Load Test Data ---\n",
    "print(\"\\n--- Loading Test Data ---\")\n",
    "try:\n",
    "    test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "    print(f\"Successfully loaded test data from {TEST_DATA_PATH}\")\n",
    "except Exception as e:\n",
    "    raise IOError(f\"Error loading test data file: {e}\")\n",
    "\n",
    "# --- 3. Optional: Data Augmentation ---\n",
    "print(\"\\n--- Optional: Data Augmentation ---\")\n",
    "# EXAMPLE: If your model was trained on a filtered 'Power' column, create it here.\n",
    "# You can call this function multiple times for different columns.\n",
    "# test_df = apply_butterworth_filter(\n",
    "#     df=test_df,\n",
    "#     column_name='Power', \n",
    "#     new_column_name='Watts_fltr_3e-3', \n",
    "#     filter_order=2, \n",
    "#     sampling_freq=10, # Hz\n",
    "#     corner_freq=0.003 # Hz\n",
    "# )\n",
    "print(\"Skipping data augmentation. Uncomment and edit the example above if needed.\")\n",
    "\n",
    "# --- 4. Load Scaler and Validate Columns ---\n",
    "print(\"\\n--- Loading Scaler and Validating Columns ---\")\n",
    "scaler = None\n",
    "normalized_columns = []\n",
    "\n",
    "try:\n",
    "    with open(JOB_METADATA_PATH, 'r') as f:\n",
    "        job_metadata = json.load(f)\n",
    "    if job_metadata.get('normalization_applied', False):\n",
    "        scaler = joblib.load(SCALER_PATH)\n",
    "        normalized_columns = job_metadata.get('normalized_columns')\n",
    "        print(f\"Successfully loaded scaler from {SCALER_PATH}\")\n",
    "        \n",
    "        missing_scaler_cols = [col for col in normalized_columns if col not in test_df.columns]\n",
    "        if missing_scaler_cols:\n",
    "            error_message = (f\"\\n\\nCRITICAL ERROR: The test data is missing columns required for normalization: {missing_scaler_cols}.\\n\\n\" \\\n",
    "                           \"These columns were likely generated during data augmentation in the original training run.\\n\\n\" \\\n",
    "                           \"Please use the 'Data Augmentation' cell above to create these columns before proceeding.\")\n",
    "            raise ValueError(error_message)\n",
    "        print(\"All columns required for normalization are present.\")\n",
    "    else:\n",
    "        print(\"Normalization was not applied during training. No scaler loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"job_metadata.json or scaler not found. Assuming no normalization was applied.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load scaler or metadata. Error: {e}\")\n",
    "\n",
    "# --- 5. Prepare Data for Inference ---\n",
    "print(\"\\n--- Preparing Data for Inference ---\")\n",
    "missing_feature_cols = [col for col in FEATURE_COLUMNS if col not in test_df.columns]\n",
    "if missing_feature_cols:\n",
    "    raise ValueError(f\"Error: The test data is missing required model feature columns: {missing_feature_cols}\")\n",
    "print(\"All required model feature columns are present.\")\n",
    "\n",
    "data_to_process = test_df[FEATURE_COLUMNS].copy()\n",
    "if scaler:\n",
    "    data_to_process[normalized_columns] = scaler.transform(data_to_process[normalized_columns])\n",
    "    print(\"Test data normalized using the loaded scaler.\")\n",
    "\n",
    "if TRAINING_METHOD == 'WholeSequenceFNN':\n",
    "    X_test_np = data_to_process.values\n",
    "    X_test = torch.tensor(X_test_np, dtype=torch.float32).to(DEVICE)\n",
    "    print(\"Data prepared for WholeSequenceFNN (no sequences).\")\n",
    "else:\n",
    "    if len(data_to_process) < LOOKBACK:\n",
    "        raise ValueError(f\"Test data length ({len(data_to_process)}) is less than the model's lookback window ({LOOKBACK}).\")\n",
    "    X_test_np = create_sequences(data_to_process.values, LOOKBACK)\n",
    "    X_test = torch.tensor(X_test_np, dtype=torch.float32).to(DEVICE)\n",
    "    print(f\"Data prepared with sequences of lookback {LOOKBACK}.\")\n",
    "\n",
    "# --- 6. Load Model ---\n",
    "print(\"\\n--- Loading Model ---\")\n",
    "input_size = len(FEATURE_COLUMNS)\n",
    "\n",
    "if MODEL_TYPE == 'FNN':\n",
    "    model_input_size = input_size * LOOKBACK if TRAINING_METHOD != 'WholeSequenceFNN' else input_size\n",
    "    model = FNNModel(\n",
    "        input_size=model_input_size,\n",
    "        output_size=model_hyperparams['output_size'],\n",
    "        hidden_layer_sizes_str=model_hyperparams['hidden_layer_sizes'],\n",
    "        activation_str=model_hyperparams.get('activation', 'ReLU'),\n",
    "        dropout_prob=float(model_hyperparams.get('dropout_prob', 0.0))\n",
    "    )\n",
    "elif MODEL_TYPE == 'LSTM':\n",
    "    model = LSTMModel(\n",
    "        input_size=input_size,\n",
    "        hidden_units=int(model_hyperparams['hidden_size']),\n",
    "        num_layers=int(model_hyperparams['num_layers']),\n",
    "        device=DEVICE\n",
    "    )\n",
    "elif MODEL_TYPE == 'GRU':\n",
    "    model = GRUModel(\n",
    "        input_size=input_size,\n",
    "        hidden_units=int(model_hyperparams['hidden_size']),\n",
    "        num_layers=int(model_hyperparams['num_layers']),\n",
    "        device=DEVICE\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model type: {MODEL_TYPE}\")\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f\"Successfully instantiated and loaded model state.\")\n",
    "\n",
    "# --- 7. Run Inference ---\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "with torch.no_grad():\n",
    "    predictions_normalized = model(X_test)\n",
    "predictions_normalized = predictions_normalized.cpu().numpy()\n",
    "print(f\"Inference complete. Generated {len(predictions_normalized)} predictions.\")\n",
    "\n",
    "# --- 8. Denormalize Predictions ---\n",
    "print(\"\\n--- Denormalizing Predictions ---\")\n",
    "if scaler:\n",
    "    predictions_final = inverse_transform_single_column(predictions_normalized, scaler, TARGET_COLUMN, normalized_columns)\n",
    "    print(f\"Predictions denormalized for target column '{TARGET_COLUMN}'.\")\n",
    "else:\n",
    "    predictions_final = predictions_normalized.flatten()\n",
    "    print(\"No scaler was used; predictions are in the original scale.\")\n",
    "\n",
    "# --- 9. Save Predictions ---\n",
    "print(\"\\n--- Saving Predictions ---\")\n",
    "output_df = pd.DataFrame()\n",
    "if TRAINING_METHOD == 'WholeSequenceFNN':\n",
    "    output_df = test_df.copy()\n",
    "    output_df['Predicted_Value'] = predictions_final\n",
    "else:\n",
    "    prediction_start_index = LOOKBACK - 1\n",
    "    output_df = test_df.iloc[prediction_start_index:].copy()\n",
    "    output_df = output_df.iloc[:len(predictions_final)]\n",
    "    output_df['Predicted_Value'] = predictions_final\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name_part = os.path.basename(MODEL_FOLDER_PATH)\n",
    "output_filename = os.path.join(os.path.dirname(TEST_DATA_PATH), f\"predictions_{model_name_part}_{timestamp}.csv\")\n",
    "\n",
    "try:\n",
    "    output_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Successfully saved predictions to: {output_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving predictions file: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
